{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**I will be mentioning all the important dependencies , we must know for ML**"
      ],
      "metadata": {
        "id": "fxX6IxhPHV8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7952TITHAjw"
      },
      "outputs": [],
      "source": [
        "# for array and matrices in python\n",
        "import numpy as np\n",
        "# for making a dataframe and preprocessing stuff\n",
        "import pandas as pd\n",
        "# for plotting operations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# for directly accessing the sklearn datasets\n",
        "import sklearn.datasets\n",
        "# xgbregressor for linear prediction purposes\n",
        "from xgboost import XGBRegressor\n",
        "# sklearn is an important ML python library\n",
        "from sklearn.model_selection import train_test_split\n",
        "# if our data is very random like 0.002 in one column and 501 in other we can standardise it to make better predictions\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# sorry we can import Logistic Regression model from sklearn like this\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# import the support vector machine , A support vector machine (SVM) is a supervised machine learning algorithm that classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space.\n",
        "from sklearn import svm\n",
        "# for checking the accuracy score of our model\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here I'll describe the pandas library, and various pre-processing methods we can use**"
      ],
      "metadata": {
        "id": "IClo9looIn6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we've our csv file uploaded in slider , we can just pass its path into the pandas to load that dataset into a pandas dataframe , for also including heading remove header=None\n",
        "dataset = pd.read_csv('path/of/the/dataset',header=None)\n",
        "# directly accessing the dataset from sklearn\n",
        "import_dataset = sklearn.datasets.fetch_california_housing()\n",
        "dataset = pd.DataFrame(import_dataset.data,columns=import_dataset.feature_names)\n",
        "# add the target array or the house prices to the dataframe\n",
        "house_price_dataframe['price'] = house_price_dataset.target\n",
        "\n",
        "# for getting the first five rows of the dataset , and if you pass a number in the parenthesis , this will print that many no. of rows\n",
        "dataset.head()\n",
        "# for getting the shape of the data\n",
        "dataset.shape\n",
        "# for getting the statistical parameters of the dataset - count,mean,std,min and soon\n",
        "dataset.describe()\n",
        "# for getting the different categories present in a particular column -> a good dataset has a comparable count of each category\n",
        "dataset['legend of that column'].value_counts()\n",
        "# for finding the mean for each category , we can use\n",
        "dataset.groupby('legend of that column').mean()\n",
        "# if any column is of no use , we can drop it using  ->  axis=1 for column and 0 for row\n",
        "dataset.drop(columns='legend name',axis=1)\n",
        "# for checking any missing values in the dataset\n",
        "dataset.isnull().sum()\n",
        "# only a few data are having missing values , so we'll just replace them with null/mean of other values\n",
        "# dropping all the missing values only\n",
        "dataset = dataset.dropna()\n",
        "# for encoding a column that has non-numerical categories\n",
        "dataset.replace({'legend':{'Category1':0,'Category2':1, similarly for other columns if any}},inplace=True)\n",
        "# replace the 3+ in dependents to 4 to make prediction easier\n",
        "loan_dataset = loan_dataset.replace(to_replace='3+',value=4)\n",
        "\n",
        "\n",
        "\n",
        "# Standardising the data , some values are very big and others very small , so we'll bring everything to the common ground using standardisation , fit transform on only train DS , on transform on test then\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "standardised_data = scaler.transform(x)\n",
        "# for splitting the data into test and train -> test size is the ratio of test dataset to train dataset , stratify=Y will make almost equal partitions of both categories in test as well as train,random_state should be same for similar dataset splitting of two users\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)\n"
      ],
      "metadata": {
        "id": "1vAqfl1UInfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation**"
      ],
      "metadata": {
        "id": "YNkYDDKIX7kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding the correlation between the various features in the dataset\n",
        "# 1) positive correlation , both change simultaneusly +1\n",
        "# 2) negative correlation , both change opposingly -1\n",
        "\n",
        "correlation = dataset.corr()\n",
        "# plotting a heatmap to understand the correlation\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(correlation,cbar=True,square=True,fmt='.2f',annot=True,annot_kws={'size':8},cmap='Blues')"
      ],
      "metadata": {
        "id": "qb3XRI-oYAZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**See how I will load the model and fit data in it**"
      ],
      "metadata": {
        "id": "4pDTPun2Nfma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we loaded the logisticRegression model here -> which is used for classification purposes\n",
        "model = LogisticRegression()\n",
        "# now training the model on our X_train and Y_train\n",
        "model.fit(X_train,Y_train)\n",
        "# for checking accuracy on training data , similarly we can do for test dataset and remember this is for category prediction model only\n",
        "train_data_prediction = model.predict(X_train)\n",
        "train_data_accuracy = accuracy_score(X_train,Y_train)\n",
        "\n",
        "\n",
        "\n",
        "# making the training the data using the support vector machine classifier\n",
        "classifier = svm.SVC(kernel='linear')\n",
        "classifier.fit(X_train,Y_train)\n",
        "\n",
        "\n",
        "\n",
        "# making and training the model on training set\n",
        "regressor = XGBRegressor()\n",
        "regressor.fit(X_train,Y_train)\n",
        "# Accuracy of the model on training set? No , we can't use that here , instead we'll calculate the errors\n",
        "training_data_prediction = regressor.predict(X_train)\n",
        "# R squared error\n",
        "score1 = metrics.r2_score(Y_train,training_data_prediction)\n",
        "# Mean squared error\n",
        "score2 = metrics.mean_absolute_error(Y_train,training_data_prediction)\n",
        "# these values should be as low as possible <1 and similarly we can calculate for test dataset\n"
      ],
      "metadata": {
        "id": "Uz5rVkjZNd0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can we make a Predictive System of our ML model**"
      ],
      "metadata": {
        "id": "7r8dHL6UOwjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a predictive system , given the input data\n",
        "input_data = (0.0192,0.0607,0.0378,0.0774,0.1388,0.0809,0.0568,0.0219,0.1037,0.1186,0.1237,0.1601,0.3520,0.4479,0.3769,0.5761,0.6426,0.6790,0.7157,0.5466,0.5399,0.6362,0.7849,0.7756,0.5780,0.4862,0.4181,0.2457,0.0716,0.0613,0.1816,0.4493,0.5976,0.3785,0.2495,0.5771,0.8852,0.8409,0.3570,0.3133,0.6096,0.6378,0.2709,0.1419,0.1260,0.1288,0.0790,0.0829,0.0520,0.0216,0.0360,0.0331,0.0131,0.0120,0.0108,0.0024,0.0045,0.0037,0.0112,0.0075)\n",
        "# changing the input_data to a numpy_array , to increase the efficiency and make it handy\n",
        "input_data_as_numpy_array = np.asarray(input_data)\n",
        "# reshape the numpy array as we're predicting for one instance only\n",
        "input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n",
        "prediction = model.predict(input_data_reshaped)\n",
        "print(prediction)\n",
        "\n",
        "if(prediction[0]=='R') :\n",
        "    print(\"The object is a Rock!\")\n",
        "else :\n",
        "    print(\"The object is a Mine!\")"
      ],
      "metadata": {
        "id": "8lp4RsO2O5JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting stuff with Matplotlib and seaborn**"
      ],
      "metadata": {
        "id": "Mp8oBzA2ZGgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising the actual & predicted prices\n",
        "plt.scatter(Y_train,training_data_prediction)\n",
        "plt.xlabel('Actual prices',color='b')\n",
        "plt.ylabel('Predicted prices',color='r')\n",
        "plt.title('Predicted versus Actual values')\n",
        "plt.show()\n",
        "\n",
        "# if there are categories in column we'll plot countplot else we'll plot displot->distribution plot\n",
        "sns.countplot(x='Education',hue='Loan_Status',data=loan_dataset)"
      ],
      "metadata": {
        "id": "D9lYxoMOZN_J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}